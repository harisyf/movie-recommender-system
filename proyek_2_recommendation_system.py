# -*- coding: utf-8 -*-
"""proyek-2-recommendation-system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zgkMfv5oWY56ttYpKTTB2JXUhlK4Inbo

# **Machine Learning Project Report - Haris Yafie**

Di tengah melimpahnya jumlah film yang tersedia di platform streaming digital, pengguna sering mengalami **kesulitan dalam memilih film** yang sesuai dengan selera mereka. Untuk menjawab tantangan tersebut, proyek ini membangun sebuah **Sistem Rekomendasi Film** berbasis machine learning yang bertujuan memberikan saran film yang **lebih personal dan relevan**. Proyek ini untuk pembelajaran saya dalam kursus Dicoding Machine Learning Terapan. Proyek ini adalah proyek sistem rekomendasi film. Data diperoleh dari Kaggle.com yang berjudul The Movie Dataset.

Proyek ini mengeksplorasi dua pendekatan utama:

1. **Content-Based Filtering (CBF)**  
   Memberikan rekomendasi berdasarkan kemiripan konten antar film, seperti judul, genre, dan kata kunci. Cocok digunakan untuk user baru yang belum banyak memiliki histori rating (*cold-start problem*).

2. **Collaborative Filtering (CF)**  
   Memberikan rekomendasi berdasarkan pola interaksi user terhadap film, khususnya dari data rating. Terdapat dua model yang dibangun:
   - **Memory-Based CF**: menggunakan cosine similarity antar item
   - **RecommenderNet**: model neural network sederhana berbasis embedding

**Project Highlight**
- Melakukan **preprocessing dan rekayasa fitur** dari data metadata film, rating pengguna, dan keywords.
- Membangun beberapa model rekomendasi dan mengevaluasi performanya menggunakan **RMSE** serta interpretasi kualitas rekomendasi.
- Menunjukkan bahwa **CBF efektif dalam menemukan film yang mirip secara tema**, sedangkan **Memory-Based CF unggul dalam memberikan rekomendasi yang personalized**.
- **RecommenderNet** memiliki potensi untuk dikembangkan lebih lanjut, meskipun pada kondisi saat ini performanya masih perlu ditingkatkan.


ID Dicoding: harisyafie

Email: yafie345@gmail.com

# **Recommendation System: Movie**
Di era digital saat ini, pengguna layanan streaming film seperti Netflix, Disney+, maupun platform lokal dihadapkan pada ribuan pilihan film yang tersedia. Jumlah film yang sangat banyak ini justru menjadi tantangan baru — pengguna sering mengalami kebingungan dalam memilih film yang sesuai dengan selera mereka.

Sistem rekomendasi hadir sebagai solusi untuk membantu pengguna menemukan film yang relevan, menarik, dan sesuai dengan preferensi personal. Selain meningkatkan pengalaman pengguna (user experience), sistem ini juga dapat meningkatkan tingkat keterlibatan pengguna (user engagement) terhadap platform.

Proyek ini memiliki dua manfaat utama. Pertama, dari sisi pengguna, sistem ini bertujuan memberikan kemudahan dalam pemilihan film yang sesuai dengan karakteristik atau histori interaksi mereka. Kedua, dari sisi pengembang (penulis), proyek ini menjadi ajang pembelajaran untuk memahami dan menerapkan berbagai pendekatan sistem rekomendasi dalam konteks nyata.

Beberapa pendekatan yang digunakan dalam sistem rekomendasi ini mengacu pada literatur yang telah ada, seperti content-based filtering, collaborative filtering, dan hybrid approach

## **Problem Statement**
Pengguna platform streaming film sering kali mengalami kesulitan dalam memilih film yang sesuai dengan preferensi pribadi mereka. Hal ini disebabkan oleh banyaknya pilihan film yang tersedia dan kurangnya sistem yang dapat secara otomatis menyesuaikan rekomendasi berdasarkan karakteristik atau histori pengguna.

## **Initialization**
"""

import pandas as pd
import matplotlib.pyplot as plt

"""## **Data Understanding**

Sebelum membangun model prediksi, penting untuk memahami terlebih dahulu karakteristik dataset yang digunakan. Pada tahap *data understanding*, dilakukan eksplorasi terhadap struktur data, kondisi kualitas data, serta pemahaman terhadap fitur-fitur yang tersedia.

Langkah ini bertujuan untuk memastikan bahwa data yang digunakan benar-benar representatif, relevan, dan siap untuk diproses lebih lanjut dalam tahap modeling. Selain itu, melalui pemahaman awal terhadap data, potensi masalah seperti missing values, atau duplikasi dapat diidentifikasi dan ditangani dengan tepat.

### **Data Collecting**
Download the dataset dari Google Drive saya. Data dikumpulkan dari website Kaggle [The Movie Dataset](https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/data). Penggunaan Google Drive mempermudah akses data
"""

# Download Movies Metadata
!gdown 1x6Q9KfOfi-EHohGvzEO7JBt2Qvh0hVEf

# Download Rating Data
!gdown 14wZJmzAig9Z5RPB9sTp_eld9PEYNVlH2

# Download ID Link Data
!gdown 1RjGSkB05MxQmfwSIajp5VXP4RLgGkSAu

# Download Keywords Data
!gdown 1pFucaeCb7intIn9-9WOcnNVRPg3hhw46

"""### **Data Loading**
Membaca data yang telah kita download menggunakan library *pandas* dan fungsi `read_csv`

"""

# Load the movies metadata
movies_metadata = pd.read_csv('movies_metadata.csv')

# Load the ratings data
ratings = pd.read_csv('ratings_small.csv')

# Load the links data
links = pd.read_csv('links_small.csv')

# Load the keywords data
keywords = pd.read_csv('keywords.csv')

movies_metadata

ratings

links

keywords

"""### **Data Checking**

Melakukan pengecekan awal pada tiap dataset untuk melihat ukuran data, duplikasi, dan nilai yang hilang (missing values).

Output Penjelasan:

- Tampilkan nama dataset

- Info jumlah baris & kolom

- Jumlah baris duplikat

- Daftar kolom yang memiliki missing values (kalau ada)
"""

# Dictionary for results
datasets = {
    "movies_metadata": movies_metadata,
    "ratings": ratings,
    "links": links,
    "keywords": keywords
}

# Loop and Print Data Checking Results
for name, df in datasets.items():
    print(f"Dataset: {name}")
    print(f"- Shape: {df.shape[0]} rows × {df.shape[1]} columns")
    print(f"- Duplicated Rows: {df.duplicated().sum()}")
    print("\nMissing Values:")
    display(pd.DataFrame(df.isnull().sum(), columns=["Missing Count"]).query("`Missing Count` > 0"))
    print("-" * 50)

"""**Interpretasi Hasil Data Checking**

`movies_metadata`
- Ukuran dataset besar (45K+ baris, 24 kolom), tapi ada **13 baris duplikat** yang perlu dihapus.
- Banyak **missing values** di beberapa kolom:
  - `belongs_to_collection`, `homepage`, dan `tagline` punya missing values yang sangat tinggi (>50%) → pertimbangkan untuk di-drop atau diabaikan tergantung relevansi.
  - Kolom penting seperti `release_date`, `runtime`, dan `overview` juga punya missing → perlu dicek apakah bisa diimputasi atau dibuang.
- Kolom numerik seperti `popularity`, `revenue`, `vote_average`, dll memiliki sedikit missing → bisa diimputasi (mean/median) atau drop.

`ratings`
- Tidak ada missing values maupun duplikat.
- Dataset bersih dan siap diproses.

`links`
- Tidak ada duplikat.
- Terdapat **13 missing values** pada kolom `tmdbId` → bisa dicek kembali relevansinya sebelum dibuang.

`keywords`
- Ada **987 baris duplikat** → perlu dihapus untuk mencegah data redundancy.
- Tidak ada missing values.

### **Exploratory Data Analysis**

EDA dilakukan untuk memahami struktur dan karakteristik awal dari setiap dataset.  
Tahapan ini penting untuk menemukan pola, insight, dan potensi masalah sebelum masuk ke tahap pemodelan.

EDA dilakukan secara terpisah untuk tiap dataset sebagai berikut:

---

#### 1. `Movies Metadata` Dataset
- **Visualisasi Genre Populer:**  
  Menggunakan bar chart untuk menampilkan genre film yang paling sering muncul.

#### 2. `Rating` Dataset
- **Statistik Deskriptif:**  
  Menghitung nilai minimum, maksimum, mean, dan distribusi rating.
- **Visualisasi Distribusi Rating:**  
  Membuat histogram untuk melihat persebaran nilai rating yang diberikan oleh user.

#### 3. `Links` Dataset
- **Analisis ID Unik:**  
  Mengecek keberagaman dan kelengkapan ID film dari berbagai sumber (IMDb, TMDb, dll).

#### 4. `Keywords` Dataset
- **Top 20 Keyword Terpopuler:**  
  Menampilkan 20 keyword yang paling sering digunakan.
- **Visualisasi Word Cloud:**  
  Membuat visualisasi cloud untuk menggambarkan frekuensi kata secara visual dan menarik.

---

EDA ini bertujuan memberikan insight awal untuk membantu proses data preparation dan pemodelan sistem rekomendasi film ke depannya.

#### **1. Movies Metadata Dataset**
**Visualisasi Genre Populer:**  
  Menggunakan bar chart untuk menampilkan genre film yang paling sering muncul.

##### **Data Visualization**

Langkah ini bertujuan untuk mengetahui genre film yang paling sering muncul dalam dataset.

- Data genre diekstrak dari kolom `genres` yang berupa string list dictionary.
- Hanya genre yang masuk dalam whitelist TMDB (`VALID_GENRES`) yang dihitung.
- Nilai yang tidak valid atau tidak bisa diparse akan diabaikan.
- Setelah diekstrak, list genre di-*explode* agar satu baris berisi satu genre.
- Visualisasi dilakukan dengan bar chart untuk melihat distribusi genre film yang paling umum.

Hasilnya menunjukkan genre yang paling sering dipakai dalam metadata film
"""

import ast

# Whitelist genre resmi TMDB
VALID_GENRES = {
    'Action','Adventure','Animation','Comedy','Crime','Documentary','Drama',
    'Family','Fantasy','History','Horror','Music','Mystery','Romance',
    'Science Fiction','TV Movie','Thriller','War','Western'
}

def safe_extract_genres(genre_str):
    """
    Return list of valid genre names or empty list if parsing fails.
    Only keeps names that exist in VALID_GENRES set.
    """
    try:
        parsed = ast.literal_eval(genre_str)
        if isinstance(parsed, list):
            names = [g.get('name') for g in parsed
                     if isinstance(g, dict) and
                        'name' in g and
                        g.get('name') in VALID_GENRES]
            return names
    except (ValueError, SyntaxError):
        pass
    return []  # fallback

movies_genre = movies_metadata[['genres']].copy()
movies_genre = movies_genre[movies_genre['genres'].notna()]          # keep non-null
movies_genre['genre_list'] = movies_genre['genres'].apply(safe_extract_genres)

# explode & hitung ulang
genre_exploded = movies_genre.explode('genre_list')
clean_counts   = genre_exploded['genre_list'].value_counts()

plt.figure(figsize=(12,6))
clean_counts.plot(kind='bar')
plt.title('Most Popular Genres (Clean)')
plt.xlabel('Genre')
plt.ylabel('Number of Movies')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""**Interpretasi Visualisasi Genre Film Terpopuler**

Dari hasil visualisasi genre film pada dataset `movies_metadata`, terlihat bahwa:

- **Drama** merupakan genre yang paling dominan, dengan jumlah film terbanyak, diikuti oleh **Comedy**, **Thriller**, dan **Romance**.
- Genre-genre populer tersebut cenderung mencerminkan preferensi umum industri film untuk tema-tema emosional, hiburan ringan, dan ketegangan.
- Genre seperti **Western**, **TV Movie**, dan **War** memiliki jumlah film yang relatif sedikit, menandakan bahwa genre ini lebih niche atau kurang diproduksi secara massal.
- Distribusi genre menunjukkan adanya konsentrasi pada genre-genre mainstream, sementara genre lain cenderung memiliki representasi yang jauh lebih kecil.

Insight ini dapat membantu dalam memahami tren dominan dalam industri film, serta berguna dalam pengembangan sistem rekomendasi berdasarkan genre populer.

#### **2. Rating Dataset**
- **Statistik Deskriptif:**  
  Menghitung nilai minimum, maksimum, mean, dan distribusi rating.
- **Visualisasi Distribusi Rating:**  
  Membuat histogram untuk melihat persebaran nilai rating yang diberikan oleh user.

##### **Descriptive Statistics**
"""

ratings['rating'].describe()

"""**Interpretasi Statistik Deskriptif `rating`**

Berdasarkan hasil statistik deskriptif:

- Jumlah data rating: **100.004** entri.
- **Rata-rata (mean)** rating: sekitar **3.54**, menunjukkan kecenderungan user memberikan rating cukup tinggi secara umum.
- **Median (50%)** dan **kuartil 75%** bernilai **4.0**, artinya lebih dari setengah user memberi rating ≥ 4.
- **Kuartil 25%** bernilai **3.0**, menunjukkan bahwa 75% rating berada di atas nilai ini.
- Nilai rating **berkisar antara 0.5 hingga 5.0**, dengan **standar deviasi sebesar 1.05**, mengindikasikan bahwa persebaran rating cukup terkonsentrasi di sekitar nilai tengah (3–4).

**Insight:**  
User cenderung memberikan rating yang positif terhadap film yang mereka tonton. Hal ini bisa berpengaruh pada sistem rekomendasi, karena adanya bias positif dalam persebaran rating.

##### **Data Visualization**
"""

bins = sorted(ratings['rating'].unique())

plt.figure(figsize=(10, 5))
plt.hist(ratings['rating'], bins=bins, align='left',
         rwidth=0.9, color='skyblue', edgecolor='black')
plt.title('Distribusi Rating Film (Diskret)')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.xticks(bins)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

"""**Interpretasi Histogram Rating Film**

Visualisasi menunjukkan distribusi diskret rating yang diberikan user terhadap film:

- Rating **4.0** adalah yang paling sering diberikan, diikuti oleh **3.0** dan **4.5**.
- Mayoritas rating berada pada rentang **3.0 hingga 5.0**, menandakan kecenderungan user memberi rating yang cukup tinggi.
- Rating rendah seperti **0.5 hingga 2.0** jauh lebih jarang muncul, menunjukkan bahwa user cenderung jarang memberi nilai jelek.
- Distribusi bersifat **right-skewed** (condong ke kanan), mencerminkan adanya **bias positif** dalam penilaian pengguna terhadap film.

**Insight:**  
Kecenderungan rating yang tinggi ini perlu diperhatikan dalam model rekomendasi, karena bisa menyebabkan overfitting terhadap film yang populer atau banyak dinilai positif.

#### **3. Links Dataset**
- **Analisis ID Unik:**  
  Mengecek keberagaman dan kelengkapan ID film dari berbagai sumber (IMDb, TMDb, dll).
"""

print("Unique movieId :", links['movieId'].nunique())
print("Unique imdbId  :", links['imdbId'].nunique())
print("Unique tmdbId  :", links['tmdbId'].nunique())
print("Missing tmdbId :", links['tmdbId'].isna().sum())
print("\nDtypes:\n", links.dtypes)

"""**Interpretasi EDA `links` Dataset**

Berdasarkan hasil eksplorasi:

- Jumlah nilai unik untuk `movieId` dan `imdbId` adalah **9125**, menandakan satu-ke-satu mapping yang konsisten di antara keduanya.
- `tmdbId` memiliki **9112 nilai unik**, sedikit lebih rendah dibanding yang lain.
- Terdapat **13 nilai `tmdbId` yang hilang (missing)**, sesuai dengan hasil data checking sebelumnya.
- Tipe data untuk `tmdbId` adalah `float64`, kemungkinan karena adanya nilai `NaN` (missing) yang membuatnya tidak terbaca sebagai `int64`.

**Insight:**  
`links` dataset memiliki struktur yang cukup rapi untuk menghubungkan berbagai sumber ID film.  
Namun, **13 baris dengan `tmdbId` yang hilang perlu ditangani** sebelum dilakukan merge atau pemodelan, terutama jika data dari TMDb digunakan sebagai referensi utama.

#### **4. Keywords Dataset**
Langkah ini bertujuan untuk menganalisis kata kunci (`keywords`) yang digunakan untuk mendeskripsikan film dalam dataset.

**Proses**:
- Kolom `keywords` berisi string list dari dictionary, sehingga perlu diekstrak menjadi list kata kunci menggunakan fungsi `extract_keywords`.
- Data kemudian di-*explode* agar setiap baris hanya berisi satu keyword untuk mempermudah analisis frekuensi.
- Dilakukan dua bentuk visualisasi:
  1. **Bar chart** untuk menampilkan **20 keyword paling umum** yang paling sering muncul.
  2. **Word cloud** untuk menyajikan visualisasi frekuensi keyword dalam bentuk artistik dan intuitif.
"""

def extract_keywords(kw_str):
    try:
        return [kw['name'] for kw in ast.literal_eval(kw_str)]
    except:
        return []

keywords['keyword_list'] = keywords['keywords'].apply(extract_keywords)
kw_exploded = keywords.explode('keyword_list')

"""##### **Data Visualization**"""

top_kw = kw_exploded['keyword_list'].value_counts().head(20)

plt.figure(figsize=(12, 6))
top_kw.plot(kind='bar')
plt.title('Top 20 Keywords Paling Umum')
plt.xlabel('Keyword')
plt.ylabel('Jumlah Film')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""**Interpretasi Grafik Top 20 Keywords Film**

Grafik menunjukkan 20 kata kunci (`keywords`) yang paling sering muncul dalam deskripsi film:

- **"woman director"** menempati posisi teratas dengan lebih dari 3.000 film, menandakan bahwa pencantuman gender sutradara sebagai keyword cukup umum dilakukan — bisa jadi bagian dari tag identitas atau representasi.
- Keyword seperti **"independent film"**, **"murder"**, dan **"based on novel"** menggambarkan karakteristik umum dari produksi atau cerita film.
- Banyak keyword yang berkaitan dengan tema konten dewasa atau emosional seperti **"sex"**, **"violence"**, **"nudity"**, dan **"revenge"**, yang bisa menunjukkan kecenderungan film untuk menonjolkan sisi konflik dan intensitas.
- Keyword seperti **"love"**, **"friendship"**, dan **"teenager"** juga muncul, memperkuat bahwa relasi dan dinamika usia adalah tema-tema yang sering dibawa.
- Kehadiran keyword seperti **"sequel"** dan **"duringcreditsstinger"** juga menandakan banyaknya film berformat waralaba atau cinematic universe.

**Insight:**  
Keyword memberi gambaran yang cukup kuat terhadap **tema, tone, dan gaya produksi** dari film. Hal ini sangat berguna sebagai fitur konten dalam sistem rekomendasi berbasis content, terutama ketika digabungkan dengan metadata lain seperti genre dan overview.

"""

from wordcloud import WordCloud

all_kw_text = ' '.join(kw_exploded['keyword_list'].dropna())
wc = WordCloud(width=800, height=400, background_color='white').generate(all_kw_text)

plt.figure(figsize=(15, 7))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud dari Keywords Film', fontsize=18)
plt.tight_layout()
plt.show()

"""**Interpretasi Word Cloud dari Keywords Film**

Word cloud memberikan gambaran visual terhadap kata kunci yang paling sering digunakan dalam deskripsi film:

- Keyword yang paling menonjol seperti **"woman director"**, **"independent film"**, **"murder"**, dan **"based on novel"** muncul dengan ukuran huruf paling besar, menandakan frekuensi kemunculan yang tinggi.
- Banyak kata kunci yang merepresentasikan tema cerita (misalnya: *death*, *love*, *revenge*, *friendship*, *ghost*, *suicide*), tempat (*new york*, *paris*, *york city*), bahkan karakter spesifik (*serial killer*, *teenager*).
- Kata-kata yang berkaitan dengan genre, gaya produksi, dan situasi juga terlihat, seperti *biography*, *high school*, *war*, *nudity*, *martial arts*, dan *escape*.

**Insight:**
- Word cloud ini menunjukkan **keragaman tema dan karakteristik film** dalam dataset.
- Keyword seperti ini sangat berguna dalam sistem rekomendasi berbasis konten (content-based), karena bisa mencerminkan kesamaan makna antar film yang tidak bisa dilihat hanya dari genre atau rating saja.

Word cloud ini juga memudahkan identifikasi **tema-tema dominan dan tren cerita** yang bisa jadi menarik buat segmentasi pengguna tertentu.

## **Data Preparation**

Tahap *Data Preparation* bertujuan untuk membersihkan dan mengolah data agar siap digunakan dalam proses modeling sistem rekomendasi. Berdasarkan alur kode, proses ini terdiri dari empat tahap utama:

---

### 1. Data Cleaning
### 2. Sanity Check
### 3. Merge Dataset
### 4. Final Feature Datasets (CB & CF)

### **Data Cleaning**

Langkah ini fokus pada konsistensi format dan penghapusan data yang tidak valid. Beberapa aksi utama yang dilakukan:

- **Konsistensi Kolom ID:**
  - Kolom `id` pada `movies_metadata` dikonversi menjadi numerik agar cocok dengan `tmdbId` di dataset `links`.
  - Nilai `NaN` pada kolom ID dihapus sebelum konversi tipe data.

- **Penghapusan Duplikat:**
  - Duplikat baris berdasarkan kolom `id` pada `movies_metadata` dihapus dengan menyimpan film dengan `vote_count` tertinggi.
  - Dataset `ratings`, `links`, dan `keywords` juga dibersihkan dari baris duplikat.

- **Pembersihan Kolom `genres`:**
  - Parsing nilai `genres` menjadi list of strings dan hanya menyimpan genre yang valid sesuai whitelist (misalnya: `'Drama'`, `'Comedy'`, dll).

- **Pembersihan Kolom `keywords`:**
  - Parsing data JSON-like menjadi string yang digabung dan di-lowercase agar lebih mudah digunakan pada content-based filtering.

- **Penanganan Missing Values:**
  - Baris yang tidak memiliki `title` dihapus.
  - Kolom-kolom dengan proporsi missing value lebih dari 30% dihapus dari dataset.

---
"""

# DATA CLEANING
# =====================

import ast

# ID column consistency
# =====================

# movies_metadata['id'] comes as string, sometimes '19995', sometimes '19995.0', sometimes 'tt123…'
movies_metadata['id'] = pd.to_numeric(movies_metadata['id'], errors='coerce')
movies_metadata.dropna(subset=['id'], inplace=True)
movies_metadata['id'] = movies_metadata['id'].astype(int)       # => will match links['tmdbId']
links.dropna(subset=['tmdbId'], inplace=True)                   # just in case
links['tmdbId'] = links['tmdbId'].astype(int)


# Remove exact duplicate rows
# =====================

movies_metadata = (
    movies_metadata
      .sort_values(['id', 'vote_count'], ascending=[True, False])
      .drop_duplicates(subset='id', keep='first')
)
ratings.drop_duplicates(inplace=True)
links.drop_duplicates(inplace=True)

# Only keep the first occurrence of each movie id
keywords = keywords.drop_duplicates(subset='id', keep='first')


# Clean `genres`
# =====================

VALID_GENRES = {
    'Action','Adventure','Animation','Comedy','Crime','Documentary','Drama',
    'Family','Fantasy','History','Horror','Music','Mystery','Romance',
    'Science Fiction','TV Movie','Thriller','War','Western'
}

def safe_extract_genres(genre_str):
    """Return list of valid genre names or empty list if parsing fails."""
    try:
        parsed = ast.literal_eval(genre_str)
        if isinstance(parsed, list):
            return [g.get('name') for g in parsed
                    if isinstance(g, dict) and g.get('name') in VALID_GENRES]
    except (ValueError, SyntaxError):
        pass
    return []

movies_metadata['genres'] = movies_metadata['genres'].apply(safe_extract_genres)


# Clean `keywords`
# =====================

def extract_keywords(kw_str):
    """Parse JSON-like keyword column and return a space-joined string."""
    try:
        parsed = ast.literal_eval(kw_str)
        if isinstance(parsed, list):
            words = [k['name'] for k in parsed if isinstance(k, dict) and 'name' in k]
            return ' '.join(words).lower()
    except (ValueError, SyntaxError):
        pass
    return ''

keywords['keywords'] = keywords['keywords'].apply(extract_keywords)


# Missing-value strategy
# =====================

# Drop rows missing *critical* info
movies_metadata.dropna(subset=['title'], inplace=True)

# Optionally drop cols with >30 % NaN
missing_ratio = movies_metadata.isnull().mean()
cols_to_drop  = missing_ratio[missing_ratio > 0.30].index
movies_metadata.drop(columns=cols_to_drop, inplace=True)

print("Data cleaning complete")

"""### **Sanity Check**

Tahap ini memastikan bahwa hasil *cleaning* berjalan dengan benar. Beberapa validasi yang dilakukan:

- Memastikan tidak ada ID film yang duplikat.
- Mengecek apakah kolom penting seperti `title` dan `genres` tidak mengandung missing value.
- Validasi tipe data (`genres` bertipe list dan `keywords` bertipe string).
- Memastikan bahwa semua genre berada dalam daftar whitelist yang ditentukan.

Jika ada keanehan seperti tipe data tidak sesuai atau genre tak dikenal, proses akan dihentikan melalui `assert`.


"""

# Sanity Check
# =====================

def sanity_cleaning(movies_df, kw_df):
    # 1. Row count & duplicates (by primary key)
    print("movies rows:", movies_df.shape[0])
    print("dup movie IDs:", movies_df.duplicated(subset='id').sum())
    assert movies_df.duplicated(subset='id').sum() == 0, "Dup movie IDs found!"

    # 2. Critical NaNs
    crit_cols = ['title', 'genres']  # add more if you rely on them later
    print("\nNaNs in critical cols:")
    print(movies_df[crit_cols].isna().sum())
    assert movies_df[crit_cols].isna().sum().sum() == 0, "Missing critical values!"

    # 3. Data-type expectations
    assert movies_df['genres'].map(type).eq(list).all(), "`genres` not list everywhere"
    assert kw_df['keywords'].map(type).eq(str).all(), "`keywords` not str everywhere"

    # 4. Genre whitelist
    bad_genres = (
        movies_df['genres'].explode().dropna()
           .loc[lambda g: ~g.isin(VALID_GENRES)]
    )
    assert bad_genres.empty, f"Unexpected genres: {bad_genres.unique()}"

    print("\nCleaning sanity checks passed!")

sanity_cleaning(movies_metadata, keywords)

"""### **Merge Datasets**

Setelah data bersih, dilakukan proses penggabungan dataset agar siap untuk modeling:

- `movies_metadata` digabung dengan `keywords` berdasarkan kolom `id`.
- Hasilnya kemudian digabung lagi dengan `links` untuk mendapatkan kolom `movieId` sebagai key utama yang digunakan baik untuk CB maupun CF.
- Kolom `tmdbId` dihapus karena `movieId` akan digunakan sebagai *primary key* universal.
- Dataset `ratings` tetap dipisah karena hanya digunakan pada collaborative filtering.

---
"""

# Merge Dataset

# Merge movies_metadata  keywords  (both keyed by 'id')
movies_kw = pd.merge(
    movies_metadata,
    keywords[['id', 'keywords']],
    how='left',
    on='id'
)

# Bring in movieId (from links) so CB & CF share the same primary key
movies_kw_links = pd.merge(
    movies_kw,
    links[['movieId', 'tmdbId']],
    how='inner',
    left_on='id',
    right_on='tmdbId'
).drop(columns=['tmdbId'])   # we keep movieId as the master key

# Ratings stays separate for CF
print("movies_kw_links shape:", movies_kw_links.shape)
print("ratings shape        :", ratings.shape)

"""### **Final Feature Dataset**

Dua dataset akhir disiapkan berdasarkan pendekatan yang digunakan:

- **Content-Based Filtering (CB):**
  - Disusun dari `movies_metadata`, `keywords`, dan `links`, menghasilkan dataframe `cb_movies`.
  - Fitur yang digunakan mencakup `title`, `genres`, `keywords`, `popularity`, `vote_average`, `vote_count`, dan `release_year` (diambil dari `release_date`).

- **Collaborative Filtering (CF):**
  - Menggunakan user-item rating matrix dari dataset `ratings`, menghasilkan dataframe `cf_ratings` berisi `userId`, `movieId`, dan `rating`.

Kedua dataframe ini akan digunakan sebagai basis untuk membangun sistem rekomendasi berbasis konten dan kolaboratif.
"""

# Final Feature Sets

# content-based dataframe
cb_movies = movies_kw_links[
    ['movieId', 'title', 'genres', 'keywords', 'overview', 'popularity',
     'vote_average', 'vote_count', 'release_date']
].copy()

# make release_year numeric
cb_movies['release_year'] = (
    pd.to_datetime(cb_movies['release_date'], errors='coerce')
      .dt.year
)

# collaborative-filtering dataframe
cf_ratings = ratings[['userId', 'movieId', 'rating']].copy()

print("Content Based Feature Dataset:", cb_movies.shape)
print("Collaborative Filtering Feature Dataset:", cf_ratings.shape)

cb_movies

cf_ratings

"""## **Model Development**

Pada tahap ini, kita membangun dan mengembangkan dua pendekatan utama dalam sistem rekomendasi film, yaitu **Content-Based Filtering (CBF)** dan **Collaborative Filtering (CF)**. Masing-masing pendekatan dikembangkan melalui tahapan yang sistematis mulai dari *model building*, *training*, hingga *evaluation* untuk mengukur performanya.

- **Content-Based Filtering** merekomendasikan film berdasarkan kemiripan konten seperti genre, judul, dan kata kunci dari film yang pernah disukai user.
- **Collaborative Filtering** fokus pada pola interaksi user terhadap film, yaitu menggunakan data rating sebagai dasar untuk memprediksi preferensi.

Selain membangun model, tahap ini juga mencakup proses evaluasi untuk membandingkan performa antar pendekatan. Hasil evaluasi akan menjadi dasar dalam menentukan model mana

### **Content Based Filtering**

Content-Based Filtering (CBF) adalah pendekatan rekomendasi yang berfokus pada karakteristik konten dari setiap item. Dalam konteks ini, kita menggunakan informasi dari film seperti **judul**, **kata kunci (keywords)**, dan **genre** untuk menentukan kemiripan antar film. Ide dasarnya: jika user menyukai sebuah film, maka mereka kemungkinan juga akan menyukai film lain yang memiliki konten serupa.

Pendekatan ini tidak memerlukan data dari user lain dan bisa bekerja dengan baik bahkan ketika user baru hanya menyukai satu atau dua film — cocok untuk mengatasi masalah *cold-start* pada user. Di bagian ini, kita akan membangun dua model content-based dengan teknik vektorisasi berbeda, lalu membandingkan hasil rekomendasinya.

#### **Model Building**

Pada tahap ini, model content-based dibangun dengan cara menggabungkan beberapa fitur penting dari setiap film, yaitu:
- **Judul film (`title`)**
- **Kata kunci (`keywords`)**
- **Genre (`genres`)**

Fitur-fitur tersebut digabungkan ke dalam satu kolom teks `combined_features`, yang nantinya akan digunakan sebagai dasar dalam proses perhitungan kesamaan antar film.

Langkah-langkah utama dalam pembangunan model:
1. **Preprocessing**: Menggabungkan dan menormalkan teks dari kolom fitur (diubah ke lowercase dan digabung jadi satu string).
2. **Vectorization**: Mengubah teks menjadi representasi numerik menggunakan dua teknik berbeda:
   - `TF-IDF Vectorizer` untuk menangkap bobot pentingnya kata.
   - `CountVectorizer` untuk menghitung frekuensi kata.
3. **Similarity Computation**: Menghitung **cosine similarity** antar semua film berdasarkan hasil vektorisasi.
4. **Rekomendasi**: Dibuat fungsi `get_recs()` yang akan mengembalikan Top-N film yang paling mirip dengan film input berdasarkan skor similarity-nya.

Model yang dihasilkan bersifat fleksibel karena dapat diganti-ganti jenis vectorizer-nya sesuai kebutuhan. Proses ini menghasilkan sistem rekomendasi berbasis konten yang tidak bergantung pada data dari user lain.

Library
"""

# Vectorisers & similarity
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

"""Configuration"""

# CONFIG
TOP_N = 10          # number of recommendations to return

"""Combined Features"""

# create combined_features:

cb_movies["genres"] = cb_movies["genres"].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))

# combined text features
cb_movies["combined_features"] = (
    cb_movies["title"].fillna('') + " "
    + cb_movies["keywords"].fillna('') + " "
    + cb_movies["genres"].fillna('')
).str.lower()

"""Model Function"""

# CONTENT-BASED MODELS
# ============================================================
def _build_cb_model(vectorizer, movies_df):
    """Fit vectoriser, compute cosine-sim matrix, return helper fn."""
    X = vectorizer.fit_transform(movies_df["combined_features"])
    sim_matrix = cosine_similarity(X, X, dense_output=False)

    # Map index ⇄ movieId for quick lookup
    idx_to_mid = dict(enumerate(movies_df["movieId"]))
    mid_to_idx = {v: k for k, v in idx_to_mid.items()}

    def get_recs(title, top_n=TOP_N):
        """Return (movieId, title, score) for the *top_n* similar films."""
        if title not in movies_df["title"].values:
            raise ValueError("Title not found!")
        idx = movies_df[movies_df["title"] == title].index[0]
        sim_scores = list(enumerate(sim_matrix[idx].toarray().ravel()))
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
        # skip itself (index 0)
        top = [(idx_to_mid[i], movies_df.loc[i, "title"], score)
               for i, score in sim_scores[1:top_n + 1]]
        return pd.DataFrame(top, columns=["movieId", "title", "similarity"])

    return get_recs

"""#### **Model Computation**

Setelah proses pembangunan model disiapkan, tahap selanjutnya adalah melakukan komputasi vektor dan matriks kesamaan antar film. Di bagian ini, dua model content-based dikonstruksi menggunakan pendekatan berbeda untuk vectorization:

- **TF-IDF Vectorizer**  
  Mengubah teks menjadi vektor berdasarkan *Term Frequency–Inverse Document Frequency*, sehingga kata-kata yang umum diabaikan dan kata-kata unik tiap film diberi bobot lebih tinggi.  
  Selain itu, digunakan `ngram_range=(1, 2)` untuk mempertimbangkan unigram dan bigram.

- **CountVectorizer**  
  Mengubah teks menjadi representasi vektor sederhana berdasarkan jumlah kemunculan kata (frekuensi), tanpa mempertimbangkan bobot pentingnya.

Masing-masing vectorizer dimasukkan ke dalam fungsi `_build_cb_model()` untuk menghasilkan dua model rekomendasi:
- `get_recs_tfidf` → model berbasis TF-IDF
- `get_recs_count` → model berbasis CountVectorizer

Kedua model ini siap digunakan untuk menghitung kesamaan antar film dan menghasilkan rekomendasi berbasis konten yang berbeda gaya pendekatannya.

"""

# TF-IDF + Cosine
tfidf_vec = TfidfVectorizer(stop_words="english", ngram_range=(1, 2))
get_recs_tfidf = _build_cb_model(tfidf_vec, cb_movies)

# CountVectorizer + Cosine
count_vec = CountVectorizer(stop_words="english")
get_recs_count = _build_cb_model(count_vec, cb_movies)

"""#### **Model Evaluation**

Evaluasi model Content-Based Filtering dilakukan menggunakan metode sederhana namun efektif yang disebut **sanity check**. Tujuannya adalah untuk melihat apakah model mampu merekomendasikan film yang relevan berdasarkan preferensi user.

Langkah-langkah evaluasinya:
1. **Ambil user sample** (misalnya `user_id = 45`) dan cari film yang mereka beri rating tinggi (≥ 4.0).
2. **Pilih satu film acak** dari daftar film yang disukai tersebut sebagai referensi (`ref_title`).
3. **Gunakan model rekomendasi** untuk mencari Top-N film yang mirip secara konten dengan film referensi.
4. **Bandingkan hasil rekomendasi** dari dua model:
   - TF-IDF + Cosine Similarity
   - CountVectorizer + Cosine Similarity

Hasil evaluasi ditampilkan dalam bentuk tabel yang mencantumkan:
- Film-film yang disukai user
- Rekomendasi film dari masing-masing model
- Genre dan skor similarity dari setiap film yang direkomendasikan

Meskipun sederhana, metode ini cukup efektif untuk memastikan bahwa model mampu menangkap kesamaan konten yang relevan dengan preferensi pengguna.


"""

from IPython.display import display

# sanity-check helper
# ------------------------------------------------------------
def sanity_check_cb(user_id,
                    get_recs_fn,
                    movies_df,
                    ratings_df,
                    k=10,
                    threshold=4.0,
                    random_state=42):
    liked_df = (
        ratings_df[(ratings_df["userId"] == user_id) & (ratings_df["rating"] >= threshold)]
        .merge(movies_df[["movieId", "title", "genres"]], on="movieId", how="left")
    )
    if liked_df.empty:
        raise ValueError(f"User {user_id} has no movies rated ≥ {threshold}")

    ref_title = liked_df.sample(1, random_state=random_state)["title"].values[0]

    rec_df = (
        get_recs_fn(ref_title, top_n=k)
        .merge(movies_df[["movieId", "genres"]], on="movieId", how="left")
    )

    return ref_title, rec_df, liked_df.sort_values("rating", ascending=False)

# run sanity-check for both CB models
# ------------------------------------------------------------
user_id = 45
k = 10
threshold = 4.0

ref_title, tfidf_recs, liked = sanity_check_cb(
    user_id, get_recs_tfidf, cb_movies, cf_ratings, k, threshold
)

_, count_recs, _ = sanity_check_cb(       # same ref_title via k=0 trick
    user_id, get_recs_count, cb_movies, cf_ratings, k, threshold
)

# Pretty print
# ------------------------------------------------------------
print(f"\nReference film for user {user_id}: '{ref_title}'\n")

print("Films this user rated ≥ 4:")
display(liked[["movieId", "title", "genres", "rating"]].head(20))

print("\nTop-10 recommendations – TF-IDF model:")
display(tfidf_recs[["movieId", "title", "genres", "similarity"]])

print("\nTop-10 recommendations – CountVectorizer model:")
display(count_recs[["movieId", "title", "genres", "similarity"]])

"""**Interpretasi Evaluasi Model**

Berdasarkan hasil evaluasi menggunakan user dengan ID `45`, model merekomendasikan film yang memiliki kemiripan konten dengan salah satu film favorit user, yaitu **"Beauty and the Beast"**.

User ini memiliki preferensi yang cukup jelas terhadap genre **Drama**, **Romance**, dan beberapa elemen **Fantasy**, **Thriller**, dan **Comedy**. Hal ini terlihat dari daftar film yang mereka beri rating tinggi, seperti *Vertigo*, *Boogie Nights*, *City Lights*, dan *Talk to Her*.

##### TF-IDF Model
Rekomendasi dari model TF-IDF menampilkan film-film yang cukup relevan secara tematik dan emosional, seperti:
- *Stealing Beauty*
- *Stage Beauty*
- *Dangerous Beauty*

Meskipun skor similarity relatif kecil (di bawah 0.25), model tetap mampu menyarankan film yang berkaitan erat dengan kata kunci "beauty" dan nuansa romantis. TF-IDF cenderung lebih fokus pada **kata-kata unik dan bobot tematik**.

##### CountVectorizer Model
Model ini memberikan skor similarity yang lebih tinggi secara numerik (hingga > 0.5), dengan film-film yang hampir identik secara kata seperti:
- *Stealing Beauty*
- *Stage Beauty*
- *Dangerous Beauty*

Hasilnya sangat mirip dengan TF-IDF dalam hal judul, tetapi lebih literal karena CountVectorizer hanya menghitung **frekuensi kata**. Model ini lebih mudah "terpengaruh" oleh kemiripan kata secara eksplisit daripada bobot pentingnya.

##### Insight
- Kedua model berhasil menemukan film-film yang berada di **jalur preferensi user**, terutama dalam tema romantis dan drama.
- TF-IDF bekerja baik untuk menangkap nuansa dan bobot kata, meski skor similarity-nya lebih kecil.
- CountVectorizer lebih sensitif terhadap kata yang sering muncul, menghasilkan skor similarity yang lebih tinggi tapi juga lebih "kasar" dalam interpretasi.

Secara keseluruhan, hasil ini menunjukkan bahwa **model content-based dapat memberikan rekomendasi yang relevan** dengan memanfaatkan informasi konten film, bahkan tanpa data interaksi user lain.

**Sehingga model yang digunakan yaitu TF-IDF Vectorized**

### **Collaborative Filtering**

Collaborative Filtering (CF) adalah pendekatan sistem rekomendasi yang berdasarkan pada pola interaksi pengguna terhadap item, bukan pada konten dari item itu sendiri. Dalam konteks proyek ini, CF memanfaatkan **data rating dari user terhadap film** untuk mempelajari pola preferensi yang serupa antar pengguna atau antar film.

Ide utamanya: *"Jika dua user menyukai film yang sama, maka mereka cenderung akan menyukai film lainnya yang juga disukai oleh user dengan preferensi serupa."*

CF sangat efektif untuk menghasilkan rekomendasi **personalized** karena benar-benar didasarkan pada kebiasaan dan interaksi nyata dari pengguna. Namun, pendekatan ini juga memiliki tantangan seperti:
- *Cold-start problem* untuk user baru tanpa histori rating
- *Sparsity problem* karena kebanyakan user hanya memberi rating ke sedikit film

Dalam proyek ini, dua pendekatan CF akan digunakan:
1. **Memory-Based Collaborative Filtering** – Menghitung kesamaan antar item berdasarkan user-item matrix.
2. **Model-Based Collaborative Filtering** dengan neural network sederhana (*RecommenderNet*).

Keduanya akan dibandingkan dari sisi hasil rekomendasi dan performa metrik.

#### **Model Building**

Pada Collaborative Filtering, kita membangun dua jenis model berdasarkan data rating antar user dan film:

---

**1. Memory-Based Collaborative Filtering**

Pendekatan ini menggunakan **item-item similarity** berdasarkan pola rating yang diberikan oleh user. Langkah-langkah utamanya:

- Membuat **user-item matrix** dari data rating.
- Mengubahnya menjadi matriks sparse (`csr_matrix`) untuk efisiensi komputasi.
- Menghitung **cosine similarity** antar film berdasarkan rating user.
- Mengembangkan fungsi `get_recs_memory()` untuk memberikan rekomendasi film kepada user berdasarkan rating tinggi dari film serupa yang pernah ditonton.

Pendekatan ini mudah diimplementasikan dan tidak memerlukan proses training model, tetapi bisa terkena masalah sparsity jika banyak film belum memiliki cukup rating.

---

**2. Model-Based Collaborative Filtering (RecommenderNet)**

Model ini menggunakan pendekatan neural network sederhana untuk mempelajari hubungan antara user dan film. Langkah-langkahnya:

- Melakukan encoding terhadap `userId` dan `movieId` menjadi indeks numerik.
- Membagi data menjadi **training dan validation set**.
- Membangun arsitektur **embedding layer** untuk user dan film.
- Menggunakan **dot product** dari embedding sebagai prediksi rating.
- Melatih model menggunakan *Mean Squared Error (MSE)* dan mengevaluasi dengan *Root Mean Squared Error (RMSE)*.

Model ini dikenal sebagai **RecommenderNet** dan mampu menangkap representasi laten dari user dan film, memberikan hasil yang lebih fleksibel dan akurat dalam jangka panjang, terutama untuk dataset yang besar.

---

Kedua model ini akan dibandingkan pada tahap evaluasi untuk melihat mana yang lebih optimal dalam memberikan rekomendasi personalized.

Library
"""

# CF – memory-based
from scipy.sparse import csr_matrix

# CF – model-based
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, Model
from sklearn.model_selection import train_test_split

# Vectorisers & similarity
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

"""Configuration"""

# CONFIG
TOP_N = 10          # number of recommendations to return

"""Memory Based CF Function"""

# MEMORY-BASED CF
# ================

def build_cf_memory(ratings_df):
    user_item = ratings_df.pivot_table(index="movieId",
                                       columns="userId",
                                       values="rating").fillna(0)
    sparse_matrix = csr_matrix(user_item.values)
    item_sim = cosine_similarity(sparse_matrix, dense_output=False)
    idx_to_mid = dict(enumerate(user_item.index))
    mid_to_idx = {v: k for k, v in idx_to_mid.items()}

    def get_recs_memory(user_id, top_n=TOP_N, min_rating=3.0):
        """Item-based CF: score unseen movies for *user_id*."""
        if user_id not in ratings_df["userId"].values:
            raise ValueError("User not found!")
        user_ratings = ratings_df[ratings_df["userId"] == user_id]
        seen = set(user_ratings["movieId"])
        # aggregate similarity * rating
        scores = {}
        for _, row in user_ratings.iterrows():
            if row["rating"] < min_rating:
                continue
            idx = mid_to_idx[row["movieId"]]
            sim_vec = item_sim[idx].toarray().ravel()
            for i, sim in enumerate(sim_vec):
                mid = idx_to_mid[i]
                if mid in seen or sim <= 0:
                    continue
                scores[mid] = scores.get(mid, 0) + sim * row["rating"]

        best = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_n]
        titles = cb_movies.set_index("movieId").loc[[m for m, _ in best], "title"]
        return pd.DataFrame(
            {"movieId": [m for m, _ in best],
             "title": titles.values,
             "score": [s for _, s in best]}
        )

    # expose internals for evaluation
    get_recs_memory.item_sim   = item_sim
    get_recs_memory.mid_to_idx = mid_to_idx
    return get_recs_memory

"""Recommender Net Function"""

# RECOMMENDER-NET CF
# ===================

def build_cf_recommendernet(ratings_df, embed_dim=50, epochs=10, batch=256):
    # Encode ids
    user_ids = ratings_df["userId"].unique()
    movie_ids = ratings_df["movieId"].unique()

    user2idx = {u: i for i, u in enumerate(user_ids)}
    movie2idx = {m: i for i, m in enumerate(movie_ids)}

    ratings_df["user_enc"] = ratings_df["userId"].map(user2idx)
    ratings_df["movie_enc"] = ratings_df["movieId"].map(movie2idx)

    X = ratings_df[["user_enc", "movie_enc"]].values
    y = ratings_df["rating"].values.astype(np.float32)

    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

    n_users = len(user_ids)
    n_movies = len(movie_ids)

    # --- Model
    user_in = layers.Input(shape=(), dtype="int32")
    movie_in = layers.Input(shape=(), dtype="int32")

    user_emb = layers.Embedding(n_users, embed_dim)(user_in)
    movie_emb = layers.Embedding(n_movies, embed_dim)(movie_in)

    dot = layers.Dot(axes=1)([user_emb, movie_emb])
    dot = layers.Flatten()(dot)
    out = layers.Activation("sigmoid")(dot)       # rating normalised 0-1

    model = Model([user_in, movie_in], out)
    model.compile(loss="mse",
                  optimizer="adam",
                  metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])

    history = model.fit([X_tr[:, 0], X_tr[:, 1]], y_tr,
              validation_data=([X_val[:, 0], X_val[:, 1]], y_val),
              epochs=epochs, batch_size=batch, verbose=1)

    def get_recs_nn(user_id, top_n=TOP_N):
        if user_id not in user2idx:
            raise ValueError("User not found!")
        user_enc = user2idx[user_id]
        # movies not yet rated
        seen = set(ratings_df[ratings_df["userId"] == user_id]["movieId"])
        candidates = [m for m in movie_ids if m not in seen]
        movie_encs = np.array([movie2idx[m] for m in candidates])
        preds = model.predict([np.full_like(movie_encs, user_enc), movie_encs],
                              verbose=0).ravel()
        top_idx = preds.argsort()[::-1][:top_n]
        best_mids = [candidates[i] for i in top_idx]
        titles = cb_movies.set_index("movieId").loc[best_mids, "title"]
        return pd.DataFrame(
            {"movieId": best_mids,
             "title": titles.values,
             "score": preds[top_idx]}
        )
        return pd.DataFrame(
            {"movieId": best_mids,
             "title": titles.values,
             "score": preds[top_idx]}
        )

    # expose artefacts
    get_recs_nn.model   = model
    get_recs_nn.history = history
    get_recs_nn.X_val   = X_val
    get_recs_nn.y_val   = y_val
    return get_recs_nn

"""#### **Train the Model**

Proses training dilakukan dengan memanggil dua fungsi utama:

- `build_cf_memory()` untuk membuat model Memory-Based Collaborative Filtering. Fungsi ini membentuk user-item matrix dan menghitung similarity antar item.
- `build_cf_recommendernet()` untuk melatih model neural network sederhana (RecommenderNet) berbasis embedding. Model ini dilatih menggunakan data rating user untuk mempelajari representasi laten dari user dan film.

Kedua model ini selanjutnya siap dievaluasi dan digunakan untuk menghasilkan rekomendasi personalized.

"""

get_recs_memory = build_cf_memory(cf_ratings)

get_recs_nn = build_cf_recommendernet(cf_ratings)

"""#### **Model Evaluation**

Evaluasi model Collaborative Filtering dilakukan dengan menggunakan metrik **Root Mean Squared Error (RMSE)**, yang mengukur seberapa dekat prediksi model terhadap rating aktual dari user.

Dua model dievaluasi secara terpisah:

- **Memory-Based CF**  
  Prediksi rating dihitung menggunakan kontribusi rating dari film serupa (weighted average by similarity). Nilai RMSE dihitung berdasarkan sampel acak dari data rating.

- **RecommenderNet**  
  RMSE dihitung pada data validasi yang telah dipisahkan sebelumnya. Selain itu, ditampilkan juga **learning curve** untuk memvisualisasikan perkembangan error selama training.

Akhirnya, kedua nilai RMSE dibandingkan secara visual menggunakan bar chart sederhana untuk melihat model mana yang memberikan performa prediksi terbaik.

Memory Based RMSE
"""

from sklearn.metrics import mean_squared_error

# MEMORY-BASED  RMSE
def rmse_memory(get_recs_memory_fn, ratings_df, sample_size=10_000):
    item_sim   = get_recs_memory_fn.item_sim
    mid_to_idx = get_recs_memory_fn.mid_to_idx

    def predict(u, m):
        hist = ratings_df[ratings_df.userId == u][["movieId", "rating"]]
        if hist.empty or m not in mid_to_idx:
            return np.nan
        sims, rats = [], []
        tgt = mid_to_idx[m]
        for mid, r in hist.itertuples(index=False):
            idx = mid_to_idx.get(mid)
            if idx is None or idx == tgt:
                continue
            sim = item_sim[tgt, idx]
            if sim > 0:
                sims.append(sim); rats.append(r)
        return np.dot(sims, rats)/sum(sims) if sims else np.nan

    samp = ratings_df.sample(n=min(sample_size, len(ratings_df)), random_state=42)
    true, pred = [], []
    for u, m, r, *_ in samp.itertuples(index=False):
      p = predict(u, m)
      if not np.isnan(p):
          true.append(r)
          pred.append(p)

    return np.sqrt(mean_squared_error(true, pred))

"""Recommender-Net RMSE"""

# RECOMMENDER-NET  RMSE + PLOT
def rmse_nn(get_recs_nn_fn):
    model = get_recs_nn_fn.model
    X_val, y_val = get_recs_nn_fn.X_val, get_recs_nn_fn.y_val
    preds = model.predict([X_val[:,0], X_val[:,1]], verbose=0).ravel()
    return np.sqrt(mean_squared_error(y_val, preds))

def plot_nn_history(get_recs_nn_fn):
    hist = get_recs_nn_fn.history
    plt.figure(figsize=(6,4))
    plt.plot(np.sqrt(hist.history["loss"]), label="train")
    plt.plot(np.sqrt(hist.history["val_loss"]), label="val")
    plt.ylabel("RMSE"); plt.xlabel("epoch")
    plt.title("RecommenderNet – RMSE curve"); plt.legend(); plt.tight_layout()
    plt.show()

# training-vs-val curve
plot_nn_history(get_recs_nn)

"""**Interpretation – RecommenderNet RMSE Curve**

Grafik di atas menunjukkan nilai **Root Mean Squared Error (RMSE)** pada data training dan validation selama 10 epoch pelatihan model RecommenderNet.

Berikut insight yang dapat diperoleh dari grafik tersebut:

- **Penurunan RMSE yang konsisten**: RMSE pada kedua data (train dan val) menurun tajam di awal epoch (0–3), menunjukkan bahwa model berhasil belajar dari data dan memperbaiki kesalahan prediksi secara signifikan pada fase awal pelatihan.

- **Stabil setelah epoch ke-3**: Setelah mencapai epoch ke-3, baik RMSE training maupun validation mulai stabil dan konvergen. Ini menandakan model mulai mendekati kapasitas optimalnya tanpa overfitting.

- **Gap yang kecil antara train dan val**: Jarak antara kurva training dan validation relatif kecil, yang menunjukkan bahwa model **tidak overfit** dan mampu melakukan generalisasi yang baik ke data yang belum pernah dilihat.

Kesimpulannya, model RecommenderNet menunjukkan performa yang baik dan stabil selama training, dan siap digunakan untuk menghasilkan rekomendasi personalized secara akurat.

Comparison RMSE Evaluation
"""

rmse_mem = rmse_memory(get_recs_memory, cf_ratings)
rmse_nn  = rmse_nn(get_recs_nn)

print(f"Memory-based     RMSE: {rmse_mem:.3f}")
print(f"Recommender-Net  RMSE: {rmse_nn:.3f}")

# quick comparison bar chart
plt.figure(figsize=(4,4))
plt.bar(["Memory", "RecNet"], [rmse_mem, rmse_nn])
plt.ylabel("RMSE"); plt.title("RMSE comparison"); plt.tight_layout()
plt.show()

"""**RMSE Comparison – Memory-Based vs RecommenderNet**

Grafik di atas membandingkan performa dua model Collaborative Filtering berdasarkan nilai **Root Mean Squared Error (RMSE)**:

| Model              | RMSE     |
|-------------------|----------|
| Memory-Based       | **0.931** |
| RecommenderNet     | 2.781     |

**Interpretasi:**

- **Memory-Based Collaborative Filtering** memiliki performa yang jauh lebih baik berdasarkan metrik RMSE. Nilai 0.931 menunjukkan bahwa prediksi rating yang dihasilkan cukup dekat dengan rating aktual user.
- **RecommenderNet**, meskipun menggunakan pendekatan neural network yang lebih kompleks, menghasilkan RMSE yang lebih tinggi (2.781), yang mengindikasikan bahwa prediksi ratingnya masih cukup meleset dibandingkan dengan metode memory-based.

**Insight:**

- Performa RecommenderNet kemungkinan besar bisa ditingkatkan dengan:
  - Pelatihan yang lebih panjang (lebih banyak epoch)
  - Peningkatan arsitektur model (lebih dalam atau pakai regularisasi)
  - Normalisasi data rating (misalnya skala ke 0–1)
  - Penggunaan hyperparameter tuning (batch size, learning rate, embedding size)

Namun, untuk kondisi saat ini, **model Memory-Based menjadi pilihan yang lebih baik** dalam hal akurasi prediksi rating.

> Catatan: RMSE bukan satu-satunya metrik untuk evaluasi sistem rekomendasi — metrik seperti Precision@K dan Recall@K juga penting untuk mengukur relevansi rekomendasi secara langsung.

## **Recommendation Result**

Setelah seluruh model dibangun, dikomputasi, dan dievaluasi, tahap selanjutnya adalah menghasilkan **rekomendasi film Top-N** untuk pengguna. Rekomendasi ini dihasilkan dari dua pendekatan utama:

1. **Content-Based Filtering Recommendation**  
   Memberikan rekomendasi berdasarkan kesamaan konten film (judul, genre, keywords) dengan film yang disukai oleh user.

2. **Collaborative Filtering Recommendation**  
   Memberikan rekomendasi personalized berdasarkan pola interaksi user terhadap film, baik menggunakan kemiripan antar item (memory-based) maupun representasi laten user-item (RecommenderNet).

Output dari masing-masing pendekatan berupa daftar **Top-N film** yang disarankan untuk user tertentu, disertai dengan skor kemiripan atau prediksi rating.

Rekomendasi ini dapat digunakan untuk mengevaluasi kualitas prediksi model secara kualitatif, serta melihat seberapa relevan film yang disarankan terhadap preferensi pengguna.

### **Content Based Filtering Recommendation**

Pada bagian ini, sistem akan memberikan **Top-N rekomendasi film** berdasarkan kemiripan konten dengan film yang menjadi referensi. Pendekatan yang digunakan adalah:

- **Model**: TF-IDF Vectorizer + Cosine Similarity
- **Input**: Judul film yang disukai user
- **Output**: Daftar film yang memiliki kemiripan konten tertinggi dengan film tersebut

Fungsi `recommend_cb()` digunakan untuk:
1. Mengambil film referensi berdasarkan judul input.
2. Menggunakan model content-based untuk mencari film yang mirip secara konten (judul, keyword, genre).
3. Mengembalikan rekomendasi Top-N film dengan skor similarity tertinggi, disertai informasi genre.

Contoh penggunaannya:
```python
recommend_cb("The Matrix")
"""

def recommend_cb(title, movies_df=cb_movies, model_fn=get_recs_tfidf, top_n=10):
    """
    Top-N movies recommendation based on content similarity.
    Model used: TF-IDF + Cosine Similarity.
    """
    try:
        recs = model_fn(title, top_n=top_n)
        recs = recs.merge(movies_df[["movieId", "genres"]], on="movieId", how="left")
        print(f"Rekomendasi berdasarkan film: '{title}'")
        return recs[["movieId", "title", "genres", "similarity"]]
    except Exception as e:
        print(f"Error: {e}")
        return pd.DataFrame()

recommend_cb("The Matrix")

"""**Interpretation – Content-Based Recommendation (TF-IDF)**

Rekomendasi di atas dihasilkan oleh model Content-Based Filtering dengan pendekatan **TF-IDF + Cosine Similarity**, berdasarkan film referensi: **"The Matrix"**.

**Insight dari Rekomendasi:**

- Film-film yang direkomendasikan memiliki kemiripan genre yang kuat dengan *The Matrix*, yaitu dominasi elemen **Action**, **Thriller**, dan **Science Fiction**.
- Beberapa judul seperti:
  - *The Matrix Reloaded* dan *The Matrix Revolutions* — adalah sekuel langsung dari film aslinya, sehingga kemiripannya sangat tinggi secara konten.
  - *Terminator 3*, *I, Robot*, dan *Ghost in the Shell* — mengangkat tema futuristik, kecerdasan buatan, dan perlawanan terhadap sistem, selaras dengan nuansa *The Matrix*.
- Genre yang paling sering muncul di daftar adalah **Science Fiction**, menunjukkan bahwa model berhasil menangkap genre inti dari film referensi.

**Skor Similarity:**

- Skor tertinggi dicapai oleh *The Matrix Reloaded* (0.359), diikuti *The Matrix Revolutions* (0.290), yang masuk akal mengingat keterkaitannya dalam waralaba.
- Skor similarity menurun secara bertahap, menandakan model mampu memprioritaskan film dengan kesamaan konten lebih tinggi terlebih dahulu.

**Kesimpulan:**

Model berhasil memberikan rekomendasi yang **relevan secara tematik dan genre**, dan mampu mengenali hubungan konten baik eksplisit (franchise) maupun implisit (tema dan nuansa). Ini membuktikan bahwa pendekatan content-based dapat memberikan saran yang akurat meskipun hanya bermodal satu film sebagai referensi.

### **Collaborative Filtering Recommendation**

Pada bagian ini, sistem memberikan **Top-N rekomendasi film secara personalized** untuk user tertentu menggunakan pendekatan Collaborative Filtering berbasis memori (item-item).

Fungsi `recommend_cf()` bekerja dengan dua output utama:
1. **Top-N rekomendasi film** untuk user berdasarkan pola rating terhadap film serupa.
2. **Daftar film yang sebelumnya disukai user** (rating ≥ threshold), sebagai perbandingan relevansi.

Model yang digunakan:
- **Memory-Based Collaborative Filtering**
- Pendekatan item-item dengan cosine similarity antar film

Parameter yang digunakan:
- `user_id`: ID pengguna target
- `top_n`: jumlah film yang direkomendasikan
- `threshold`: batas minimal rating yang dianggap “disukai” user (default: 4.0)

Fungsi ini berguna untuk mengevaluasi apakah model berhasil memberikan rekomendasi yang sesuai dengan selera pengguna berdasarkan histori interaksinya.

Contoh penggunaannya:
```python
recommend_cf(user_id=45)
"""

def recommend_cf(user_id,
                 ratings_df=cf_ratings,
                 movies_df=cb_movies,
                 model_fn=get_recs_memory,
                 top_n=10,
                 threshold=4.0):
    """
    Top-N personalized movies recommendation based on User ID and compare it with movies
    that have been liked by user.
    - Model used: Collaborative Filtering Memory Based (item-item).
    - Output: Two DataFrame (Recommendation and high rating historical data)
    """
    try:
        # Top-N Recommendation
        recs = model_fn(user_id, top_n=top_n)
        recs = recs.merge(movies_df[["movieId", "genres"]], on="movieId", how="left")

        # Rated Movies by user ≥ threshold
        liked_df = (
            ratings_df[(ratings_df["userId"] == user_id) & (ratings_df["rating"] >= threshold)]
            .merge(movies_df[["movieId", "title", "genres"]], on="movieId", how="left")
            [["movieId", "title", "genres", "rating"]]
            .sort_values("rating", ascending=False)
            .head(top_n)
        )


        print(f"\nRekomendasi untuk user {user_id}:")
        display(recs[["movieId", "title", "genres", "score"]])

        print(f"\nFilm yang pernah dirating tinggi oleh user {user_id}:")
        display(liked_df[["movieId", "title", "genres", "rating"]])


    except Exception as e:
        print(f"⚠️ Error: {e}")
        return pd.DataFrame(), pd.DataFrame()

recommend_cf(user_id=45)

"""#### Interpretation – Collaborative Filtering Recommendation (Memory-Based)

Rekomendasi di atas dihasilkan oleh model Collaborative Filtering berbasis memori (item-item), untuk user dengan ID `45`.

##### 🎬 Preferensi User
Berdasarkan histori rating, user 45 menunjukkan ketertarikan kuat terhadap film dengan genre:
- **Drama**, **Mystery**, dan **Thriller**
- Beberapa elemen **Romance**, **Fantasy**, dan **Sci-Fi**
- Menyukai film-film klasik dan sinematik yang kuat secara naratif

Contoh film yang disukai:
- *Vertigo*, *Beauty and the Beast*, *Boogie Nights*, *Magnolia*, *City Lights*

##### Insight dari Rekomendasi
Model berhasil memberikan rekomendasi yang sangat **selaras dengan gaya dan selera film user**, seperti:
- *Chinatown*, *L.A. Confidential*, *Citizen Kane* — film klasik dengan genre **Mystery/Drama**
- *Being John Malkovich*, *High Fidelity* — film dengan **narasi unik dan psikologis**
- *Casablanca*, *American Beauty*, *Apocalypse Now* — termasuk film **ikonik dan award-winning** dengan kedalaman karakter

##### Skor Prediksi
- Skor tertinggi mencapai **19.39**, menandakan model memiliki kepercayaan tinggi terhadap kecocokan film tersebut dengan user.
- Seluruh rekomendasi berada pada rentang skor tinggi, menunjukkan konsistensi model dalam mengenali pola preferensi user dari histori rating-nya.

##### Kesimpulan
Model Collaborative Filtering berbasis memori berhasil memberikan rekomendasi yang:
- **Personalized dan relevan secara tematik**
- Selaras dengan gaya sinematik user yang cenderung **klasik, emosional, dan naratif**
- Mampu mengenali selera user tanpa perlu mengetahui isi konten film secara eksplisit

Rekomendasi seperti ini sangat cocok untuk user aktif dengan riwayat interaksi yang cukup kaya, dan bisa meningkatkan kepuasan dalam eksplorasi film serupa.

## **Conclusion**

Pada proyek ini, kita telah berhasil membangun dan mengevaluasi dua pendekatan utama dalam sistem rekomendasi film:

---

1. **Content-Based Filtering (CBF)**  
CBF menggunakan informasi konten dari film seperti **judul**, **keywords**, dan **genre** untuk menghitung kemiripan antar film. Model ini sangat efektif digunakan saat data interaksi user masih terbatas (cold-start problem).  
Dua varian model yang dibangun:
- **TF-IDF + Cosine Similarity** — menangkap bobot penting kata
- **CountVectorizer + Cosine Similarity** — menghitung frekuensi kata

> 🔎 *Hasil evaluasi menunjukkan bahwa TF-IDF + Cosine Similarity mampu merekomendasikan film yang relevan secara tematik dengan preferensi user.*

---

2. **Collaborative Filtering (CF)**  
CF memberikan rekomendasi berdasarkan pola interaksi user dengan film. Pendekatan ini menghasilkan rekomendasi personalized yang lebih tajam seiring bertambahnya data rating.
Model yang dibangun:
- **Memory-Based CF** — menggunakan cosine similarity antar item
- **RecommenderNet** — neural network sederhana berbasis embedding

> *Hasil evaluasi menunjukkan bahwa model Memory-Based memberikan performa RMSE yang lebih baik dibandingkan RecommenderNet dalam kondisi saat ini. Namun, RecommenderNet memiliki potensi untuk dikembangkan lebih lanjut.*

---

**Final Insight**

- **CBF cocok untuk user baru**, karena tidak membutuhkan histori rating.
- **CF cocok untuk user aktif**, karena dapat memberikan rekomendasi yang lebih personal.
- Kombinasi keduanya dapat menjadi fondasi untuk **hybrid recommendation system** di masa depan.

Tahap selanjutnya dari proyek ini dapat mencakup:
- Evaluasi tambahan menggunakan **Precision@K**, **Recall@K**, dan **MAP**
- Eksplorasi **Hybrid Filtering**
- Penerapan sistem dalam bentuk API atau aplikasi interaktif

Sistem rekomendasi yang telah dibangun membuktikan bahwa pendekatan machine learning dapat secara efektif membantu user menemukan film yang sesuai dengan preferensi mereka.
"""